<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yeon-Ji Song</title>
    <meta name="author" content="YeonJi Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:5%;width:65%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Yeon-Ji Song
                    </p>
                    <p style="color: white; text-align: center;">ÏÜ°Ïó∞ÏßÄ</p>
                    <p>
                      I am a PhD candidate at the Seoul National University (<a target="_blank" href="https://en.snu.ac.kr/">SNU</a>) Biointelligence Lab, supervised by <a target="_blank" href="https://bi.snu.ac.kr/members/byoung-tak-zhang.html">Byoung-Tak Zhang</a>.
                      Previously, I graduated from the Hong Kong University of Science and Technology (<a target="_blank" href="https://seng.hkust.edu.hk/">HKUST</a>) with a Bachelor's in Electronic and Computer Engineering.
                      During my undergrad, I completed my Bachelor's thesis on Autonomous Navigation and Object Detection with Reinforcement Learning in RAM-Lab advised by Prof. Ming Liu.
                    </p>
                    <p style="text-align:center">
                      <a href="mailto:yjasz98@gmail.com">Email</a> &nbsp;/&nbsp;
                      <a target="_blank" href="data/yeonjisong-cv.pdf">CV</a> &nbsp;/&nbsp;
                      <!-- Scholar</a> &nbsp;/&nbsp; -->
                      <a href="https://github.com/yeonjisong/">Github</a>
                    </p>
                  </td>
                  <td style="padding:2%;width:80%;max-width:80%">
                    <a>
                      <img style="width:80%;height:80%; object-fit:cover; object-position:center; border-radius: 50%;" alt="profile photo" src="images/profile3.jpg" class="hoverZoomLink">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>News</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <div class="scrollable-news">
              <ul>
                <li>[2025.06] One paper is accepted at ICCV 2025!üî•</li>
                <li>[2025.05] One paper is accepted at CVPR 2025 Workshop!‚ú®</li>
                <li>[2024.12] I am selected as a Samsung Industrial-Academic Scholarship recipient!üôåüèª</li>
                <li>[2023.11] One paper is accepted at NeurIPS 2023 Workshop!‚ú®</li>
              </ul>
            </div>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <p>
                      I am interested in <i>3D vision</i>, <i>representation learning</i>, and <i>generative AI</i>. Specifically, learning representations in a self-supervised and structured way. Most of my work is about inferring the physical world from images or methods that use domain-specific knowledge of geometry and graphics. I enjoy being inspired by neuroscience to understand human intelligence of AI.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="n4dgs_stop()" onmouseover="n4dgs_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src="" style="width: 100%;">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>N4DGS: Newtonian-based Dynamic-Static Decomposition for 4D Gaussian Splatting</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Junoh Lee, Kiyoung Kwon, Jin-Hwa Kim<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>To be updated</em>
                    <!-- <br>
                    <p></p>
                    <p>
                       TBU
                    </p> -->
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <!-- <tr onmouseout="egograsp_stop()" onmouseover="egograsp_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src="images/egograsp.jpg" style="width: 100%;">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>EgoGrasp: Egocentric Object Grasping via Affordance-aware 3D Gaussian Splatting</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Hyunseo Kim, Minsu Lee<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>TBU</em>
                    <br>
                    <p></p>
                    <p>
                       TBU
                    </p>
                  </td>
                </tr> -->

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="o3ags_stop()" onmouseover="o3ags_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src="images/o3ags.jpg" style="width: 100%;">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>AffoRo-GS: Few-shot 3D Affordance Learning for Open-vocabulary Robotic Manipulation with Gaussian Splatting</a></span>
                    </a>
                    <br>
                    Hyunseo Kim, <strong><u>Yeon-Ji Song</u></strong>, Minsu Lee<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>Under review</em>
                    <br>
                    <p></p>
                    <p>
                       We introduce an efficient approach to 3D affordance learning connected to action verbs, leveraging few-shot affordance labels. Given affordance-labeled images captured from a sparse set of viewpoints, our model learns 2D visual features that are tightly aligned with affordance-related text embeddings. These visual features are subsequently uplifted into 3D Gaussians to facilitate multi-stage robotic manipulation.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="ock_stop()" onmouseover="ock_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/ock.png' width="100%">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>OCK: Unsupervised Dynamic Video Prediction with Object-Centric Kinematics</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Jaein Kim<sup>&ast;</sup>, Suhyung Choi<sup>&ast;</sup>, Jin-Hwa Kim<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>ICCV</em>, 2025
                    <br>
                    <a target="_blank" href="https://github.com/yeonjisong/ock">
                    project page</a>
                     /
                    <a href="https://arxiv.org/abs/2404.18423v3" target="_blank">arXiv</a>
                    <p></p>
                    <p>
                      We propose OCK, a dynamic video prediction model leveraging object-centric kinematics and object slots. We introduce a novel component named Object Kinematics that comprises explicit object motions, serving as an additional attribute beyond conventional appearance features to model dynamic scenes.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="dbmovi_stop()" onmouseover="dbmovi_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/dbmovi.png' width="100%">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>DBMovi-GS: Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Jaein Kim</a>, Byoungju Kim</a>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>CVPR NFBCC Workshop</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/2506.20998">arXiv</a>
                    <p></p>
                    <p>
                      We address the challenge of novel view synthesis from blurry inputs by generating dense 3D Gaussians, restoring sharpness, and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="CSEConv_stop()" onmouseover="CSEConv_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/CSEConv.png' width="100%"> 
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>Continuous SO(3) Equivariant Convolution for 3D Point Cloud Analysis</a></span>
                    </a>
                    <br>
                    Jaein Kim, Heebin Yoo, Dong-Sig Han, <strong><u>Yeon-Ji Song</u></strong>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>ECCV</em>, 2024
                    <br>
                    <a href="https://github.com/qpwodlsqp/CSEConv">code</a> / <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06843.pdf">arXiv</a>
                    <p></p>
                    <p>
                      We propose CSEConv, a novel point convolution layer equivariant under continuous SO(3) actions. Its structure is founded on the framework of group theory, realizing the convolution module defined on a sphere.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="ksc24_stop()" onmouseover="ksc24_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/ksc24.png' width="100%">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>Unsupervised Visual Dynamics Learning with Multi-Object Kinematics</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>KSC</em>, 2024 &nbsp; <font color="grey"><strong>(Best Presentation Award)</strong></font>
                    <br>
                    <p>
                      We propose a novel view synthesis method based on 3DGS that sequentially processes blurry input frames to generate new viewpoints without requiring pre-calculated camera poses. Our approach simultaneously achieves high performance in both view synthesis and camera pose estimation under dynamic conditions.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="oamd_stop()" onmouseover="oamd_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/oamd.png' width="100%">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>Learning Object Appearance and Motion Dynamics with Object-Centric Representations</a></span>
                    </a>
                    <br>
                    <strong><u>Yeon-Ji Song</u></strong>, Hyunseo Kim, Jaein Kim, Jin-Hwa Kim<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>NeurIPS CRL Workshop</em>, 2023
                    <br>
                    <!-- project page</a> / -->
                    <a href="https://openreview.net/forum?id=yzKb3uFiir">arXiv</a>
                    <p></p>
                    <p>
                      Object-centric representations have emerged as a promising tool for scene decomposition by providing useful abstractions. We propose a method called Object-centric Slot Patch Transformer, which is a dynamics learning mechanism that predicts future frames in an object-centric way.
                    </p>
                  </td>
                </tr>

                <!-- bgcolor="#ffffd0" -->
                <tr onmouseout="clear_stop()" onmouseover="clear_start()">
                  <td style="padding:16px;width:30%;vertical-align:middle">
                    <img src='images/clear.png' width="100%">
                  </td>
                  <td style="padding:8px;width:70%;vertical-align:middle">
                    <span class="papertitle"><a>On Discovery of Local Independence over Continuous Variables via Neural Contextual Decomposition</a></span>
                    </a>
                    <br>
                    Inwoo Hwang, Yunhyeok Kwak, <strong><u>Yeon-Ji Song</u></strong>, Sanghack Lee<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup>
                    <br>
                    <em>CLeaR</em>, 2023
                    <br>
                    <a href="https://openreview.net/forum?id=-aFd28Uy9td">arXiv</a>
                    <p></p>
                    <p>
                      Local independence, such as context-specific independence, has primarily been studied for discrete variables despite its importance for understanding fine-grained causal relationships. We extend this concept to continuous variables by defining its properties and introducing a differentiable method for its discovery.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:small;">
                      (Last updated: Oct 2025)
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
